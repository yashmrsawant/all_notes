## Abstract: 
Generalisation (or transfer) is the ability to repurpose knowledge in novel settings. It is often asserted that generalisation is an important ingredient of human intelligence, but its extent, nature and determinants have proved controversial. Here, we re-examine this question with a new paradigm that formalises the transfer learning problem as one of recomposing existing functions to solve unseen problems. We find that people can generalise compositionally in ways that are elusive for standard neural networks, and that human generalisation benefits from training regimes in which items are axis-aligned and temporally correlated. We describe a neural network model based around a Hebbian gating process which can capture how human generalisation benefits from different training curricula. We additionally find that adult humans tend to learn composable functions asynchronously, exhibiting discontinuities in learning that resemble those seen in child development

See similar study from the same group: [[Neural state space alignment for magnitude generalization in humans and recurrent networks]]

## References:
https://psyarxiv.com/qnpw6
